# Configuring LLM

---

Depending on your use case, you can plug in any third-party LLM—whether it’s ModelArk, MiniMax, Qwen, Stepfun, DeepSeek, or your own in-house model. This guide walks you through configuring for the above kinds of LLMs and highlights key considerations.

## Using Third-party LLMs

<Note title="Note">
Please contact ZEGOCLOUD Technical Support first to activate third-party LLM services and obtain the access Url and API Key.

Third-party LLMs must be compatible with the OpenAI protocol.
</Note>

You can set LLM parameters when registering an AI agent ([RegisterAgent](./../API%20reference/Agent%20Configuration%20Management/Register%20Agent.mdx)) or creating an AI agent instance ([CreateAgentInstance](./../API%20reference/Agent%20Instance%20Management/Create%20Agent%20instance.mdx)). Parameters are as follows:

| Parameter       | Type            | Required | Description                                                                                                                                                  |
|-----------------|-----------------|----------|--------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Url             | String          | Yes      | The endpoint that receives requests (can be your own service or a service provided by any LLM service provider), and must be compatible with the [OpenAI Chat Completions API](https://platform.openai.com/docs/api-reference/chat).<br/>For example: `https://api.openai.com/v1/chat/completions`                                                                |
| ApiKey          | String          | No       | The parameter used by the LLM service provider for authentication. Default is empty, but it must be provided in a production environment.             |
| Model           | String          | Yes      | The LLM model. Different LLM service providers support different models, please refer to their official documentation to choose an appropriate model. |
| SystemPrompt    | String          | No       | The system prompt for the AI agent. Predefined information attached at the beginning when calling the LLM, used to control the output of the LLM. It can include role settings, prompts, and response examples.   |
| Temperature     | Float           | No       | Higher values will make the output more random, while lower values will make the output more focused and deterministic.    |
| TopP            | Float           | No       | Sampling method, smaller values result in stronger determinism; larger values result in more randomness.    |
| Params          | Object          | No       | Other parameters supported by the LLM service provider, such as maximum Token number limit, etc. Different LLM providers support different parameters, please refer to their official documentation and fill in as needed. <Note title="Note">Parameter names should match those of each vendor's LLM.</Note> |

Here are configuration samples for common LLM vendors:

<Tabs>
<Tab title="ModelArk">
For model usage docs, read [ModelArk - API reference - ChatCompletions - Text Generation](https://docs.byteplus.com/en/docs/ModelArk/1298454).
```json
"LLM": {
    "Url": "POST https://ark.ap-southeast.bytepluses.com/api/v3/chat/completions",
    "ApiKey": "your_api_key",
    "Model": "ep-xxxxxxxxxx",    // Your inference access point created on the Volcano Ark Large Model Platform
    "SystemPrompt": "You are Xiao Zhi, an adult female, a **companion assistant created by ZegoCLOUD Technology**, knowledgeable in astronomy and geography, smart, wise, enthusiastic, and friendly.\nDialogue requirements: 1. Interact with users according to the character requirements.\n2. Do not exceed 100 words.",
    "Temperature": 1,
    "TopP": 0.7,
    "Params": {
        "max_tokens": 1024
    }
}
```
</Tab>
<Tab title="Qwen">
For model usage docs, read [Alibaba Cloud Model Studio - OpenAI compatibility - Chat](https://www.alibabacloud.com/help/en/model-studio/compatibility-of-openai-with-dashscope).
```json
"LLM": {
    "Url": "https://dashscope-intl.aliyuncs.com/compatible-mode/v1/chat/completions",
    "ApiKey": "your_api_key",
    "Model": "qwen-plus",
    "SystemPrompt": "You are Xiaozhi, an adult woman, a companion assistant **created by ZEGOCLOUD **. knowledgeable in everything, intelligent, wise, enthusiastic, and friendly. \nDialogue requirements: 1. Dialogue with users according to the requirements of the persona. \n2.No more than 100 words.",
    "Temperature": 1,
    "TopP": 0.7,
    "Params": {
        "max_tokens": 1024
    }
}
```
</Tab>
<Tab title="MiniMax">
For model usage docs, read [MiniMax - Chat Completion - API](https://www.minimax.io/platform/document/ChatCompletion%20v2?key=66701d281d57f38758d581d0#QklxsNSbaf6kM4j6wjO5eEek).

```json
"LLM": {
    "Url": "https://api.minimax.chat/v1/text/chatcompletion_v2",
    "ApiKey": "your_api_key",
    "Model": "MiniMax-Text-01",
    "SystemPrompt": "You are Xiaozhi, an adult woman, a companion assistant **created by ZEGOCLOUD **. knowledgeable in everything, intelligent, wise, enthusiastic, and friendly. \nDialogue requirements: 1. Dialogue with users according to the requirements of the persona. \n2.No more than 100 words.",
    "Temperature": 1,
    "TopP": 0.7,
    "Params": {
        "max_tokens": 1024
    }
}
```
</Tab>
</Tabs>

## Use Custom LLM

The ZEGOCLOUD AI Agent server uses the OpenAI API protocol to call LLM services. Therefore, you can also use a custom LLM compatible with the OpenAI protocol. The following introduces how to use a custom LLM.

<Steps>
<Step title="Create a service compatible with the OpenAI API protocol">
Provide an interface compatible with [platform.openai.com](https://platform.openai.com/docs/api-reference/chat). Key points are as follows:

- Endpoint: Define a Url that can be called by the AI Agent, for example `https://your-custom-llm-service/chat/completions`.
- Request Format: Accept request parameters compatible with the OpenAI protocol.
- Response Format: Return streaming response data that is compatible with the OpenAI protocol and conforms to the SSE specification.

<Warning title="Note">
The last valid data of the custom LLM stream must contain "finish_reason":"stop", and a termination data must be sent at the end: data: [DONE]. If these two items are missing, it may result in incomplete agent responses.
<Accordion title="Chat Completion Stream Response Object Block Example" defaultOpen="false">
```json
data: {"id":"d7ae7c4a-1524-4fe5-9d58-e4d59b89d8f0","object":"chat.completion.chunk","created":1709899323,"model":"step-1-8k","choices":[{"index":0,"delta":{"role":"","content":"He"},"finish_reason":""}],"usage":{"prompt_tokens":83,"completion_tokens":1,"total_tokens":84}}
data: {"id":"d7ae7c4a-1524-4fe5-9d58-e4d59b89d8f0","object":"chat.completion.chunk","created":1709899323,"model":"step-1-8k","choices":[{"index":0,"delta":{"role":"","content":"llo"},"finish_reason":""}],"usage":{"prompt_tokens":83,"completion_tokens":2,"total_tokens":85}}
data: {"id":"d7ae7c4a-1524-4fe5-9d58-e4d59b89d8f0","object":"chat.completion.chunk","created":1709899323,"model":"step-1-8k","choices":[{"index":0,"delta":{"role":"","content":"!"},"finish_reason":""}],"usage":{"prompt_tokens":83,"completion_tokens":3,"total_tokens":86}}
data: {"id":"d7ae7c4a-1524-4fe5-9d58-e4d59b89d8f0","object":"chat.completion.chunk","created":1709899323,"model":"step-1-8k","choices":[{"index":0,"delta":{"role":"","content":"ZEGO"},"finish_reason":""}],"usage":{"prompt_tokens":83,"completion_tokens":4,"total_tokens":87}}
data: {"id":"d7ae7c4a-1524-4fe5-9d58-e4d59b89d8f0","object":"chat.completion.chunk","created":1709899323,"model":"step-1-8k","choices":[{"index":0,"delta":{"role":"","content":"CLOUD"},"finish_reason":""}],"usage":{"prompt_tokens":83,"completion_tokens":5,"total_tokens":88}}
...
data: {"id":"d7ae7c4a-1524-4fe5-9d58-e4d59b89d8f0","object":"chat.completion.chunk","created":1709899323,"model":"step-1-8k","choices":[{"index":0,"delta":{"role":"","content":"More"},"finish_reason":""}],"usage":{"prompt_tokens":83,"completion_tokens":147,"total_tokens":230}}
data: {"id":"d7ae7c4a-1524-4fe5-9d58-e4d59b89d8f0","object":"chat.completion.chunk","created":1709899323,"model":"step-1-8k","choices":[{"index":0,"delta":{"role":"","content":"value"},"finish_reason":""}],"usage":{"prompt_tokens":83,"completion_tokens":148,"total_tokens":231}}
data: {"id":"d7ae7c4a-1524-4fe5-9d58-e4d59b89d8f0","object":"chat.completion.chunk","created":1709899323,"model":"step-1-8k","choices":[{"index":0,"delta":{"role":"","content":"."},"finish_reason":""}],"usage":{"prompt_tokens":83,"completion_tokens":149,"total_tokens":232}}
data: {"id":"d7ae7c4a-1524-4fe5-9d58-e4d59b89d8f0","object":"chat.completion.chunk","created":1709899323,"model":"step-1-8k","choices":[{"index":0,"delta":{"role":"","content":""},"finish_reason":""}],"usage":{"prompt_tokens":83,"completion_tokens":150,"total_tokens":233}}
data: {"id":"d7ae7c4a-1524-4fe5-9d58-e4d59b89d8f0","object":"chat.completion.chunk","created":1709899323,"model":"step-1-8k","choices":[{"index":0,"delta":{"role":"","content":""},"finish_reason":"stop"}],"usage":{"prompt_tokens":83,"completion_tokens":150,"total_tokens":233}}
data: [DONE]
```
</Accordion>
</Warning>
</Step>
<Step title="Configure the custom LLM">
When registering an AI agent ([RegisterAgent](./../API%20reference/Agent%20Configuration%20Management/Register%20Agent.mdx)) or creating an AI agent instance ([CreateAgentInstance](./../API%20reference/Agent%20Instance%20Management/Create%20Agent%20instance.mdx)), set the configuration for the custom LLM.

```json
"LLM": {
    "Url": "https://your-custom-llm-service/chat/completions",
    "ApiKey": "your_api_key",
    "Model": "your_model",
    "SystemPrompt": "You are Xiaozhi, an adult woman, a companion assistant **created by ZEGOCLOUD **. knowledgeable in everything, intelligent, wise, enthusiastic, and friendly. \nDialogue requirements: 1. Dialogue with users according to the requirements of the persona. \n2.No more than 100 words.",
    "Temperature": 1,
    "TopP": 0.7,
    "Params": {
        "max_tokens": 1024
    }
}

```
</Step>
</Steps>

